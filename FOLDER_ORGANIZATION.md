# Folder Organization

Clean, organized structure for the senior-scrapr project.

## ğŸ“ Root Directory Structure

```
senior-scrapr/
â”œâ”€â”€ README.md                          â† Start here
â”œâ”€â”€ FOLDER_ORGANIZATION.md             â† This file
â”œâ”€â”€ memory.md                          â† AI session history
â”œâ”€â”€ scrape_all_states.py               â† Main scraper (ACTIVE)
â”‚
â”œâ”€â”€ ğŸ“‚ current_scraped_data/           â† Today's results
â”‚   â”œâ”€â”€ AZ_seniorplace_data_*.csv      â† 1,831 Arizona listings âœ…
â”‚   â”œâ”€â”€ CA_seniorplace_data_*.csv      â† ~20K California listings ğŸ”„
â”‚   â”œâ”€â”€ NEW_CALIFORNIA_LISTINGS.csv    â† 291 new CA listings ğŸ“¥
â”‚   â””â”€â”€ README.md
â”‚
â”œâ”€â”€ ğŸ“‚ monthly_updates/                â† Automated comparison results
â”‚   â””â”€â”€ YYYYMMDD_HHMMSS/
â”‚       â”œâ”€â”€ new_listings_*.csv         â† NEW listings to import
â”‚       â”œâ”€â”€ updated_listings_*.csv     â† Existing listings to update
â”‚       â””â”€â”€ update_summary_*.json      â† Statistics
â”‚
â”œâ”€â”€ ğŸ“‚ monthly_scrapers/               â† Automation system
â”‚   â”œâ”€â”€ monthly_update_orchestrator.py
â”‚   â”œâ”€â”€ compare_california_quick.py
â”‚   â”œâ”€â”€ send_monthly_report.py
â”‚   â”œâ”€â”€ test_monthly_update.py
â”‚   â””â”€â”€ setup_monthly_scheduler.sh
â”‚
â”œâ”€â”€ ğŸ“‚ scrapers_active/                â† Current working scrapers
â”‚   â”œâ”€â”€ enhanced_seniorly_scraper.py
â”‚   â”œâ”€â”€ scrape_live_senior_place_data.py
â”‚   â”œâ”€â”€ scrape_seniorly_community_types.py
â”‚   â””â”€â”€ update_prices_from_seniorplace_export.py
â”‚
â”œâ”€â”€ ğŸ“‚ scrapers_archive/               â† Old/experimental scrapers
â”‚   â””â”€â”€ [15 archived scrapers]
â”‚
â”œâ”€â”€ ğŸ“‚ docs/                           â† Documentation
â”‚   â”œâ”€â”€ MONTHLY_UPDATE_README.md
â”‚   â”œâ”€â”€ QUICK_START_MONTHLY_UPDATES.md
â”‚   â”œâ”€â”€ SYSTEM_OVERVIEW.md
â”‚   â”œâ”€â”€ FOLDER_STRUCTURE.md
â”‚   â””â”€â”€ [other docs]
â”‚
â”œâ”€â”€ ğŸ“‚ california_expansion/           â† CA expansion project
â”‚   â”œâ”€â”€ [CA-specific scrapers]
â”‚   â”œâ”€â”€ [CA data files]
â”‚   â””â”€â”€ archive_csvs/
â”‚
â”œâ”€â”€ ğŸ“‚ wordpress_import/               â† Final import files
â”‚   â””â”€â”€ [9 CSV files ready for import]
â”‚
â”œâ”€â”€ ğŸ“‚ organized_csvs/                 â† Organized data exports
â”‚   â””â”€â”€ [103 CSV files]
â”‚
â”œâ”€â”€ ğŸ“‚ data_analysis/                  â† Analysis scripts
â”‚   â””â”€â”€ [41 Python scripts]
â”‚
â”œâ”€â”€ ğŸ“‚ data_outputs/                   â† Intermediate outputs
â”‚   â””â”€â”€ [various JSON and CSV files]
â”‚
â”œâ”€â”€ ğŸ“‚ test_scripts/                   â† Test scripts
â”‚   â””â”€â”€ [23 test Python scripts]
â”‚
â”œâ”€â”€ ğŸ“‚ tools/                          â† Utility tools
â”‚   â””â”€â”€ [11 helper scripts]
â”‚
â”œâ”€â”€ ğŸ“‚ data/                           â† Checkpoints and logs
â”‚   â”œâ”€â”€ checkpoints/
â”‚   â””â”€â”€ logs/
â”‚
â””â”€â”€ ğŸ“‚ archive/                        â† Old/inactive files
    â”œâ”€â”€ old_state_fixes/               â† Deprecated state fix scripts
    â”œâ”€â”€ old_scripts/                   â† One-off utility scripts
    â”œâ”€â”€ test_csvs/                     â† Test data files
    â””â”€â”€ logs/                          â† Old log files
```

## ğŸ¯ Quick Navigation

### I want to...

**Scrape a state:**
â†’ `python3 scrape_all_states.py --states AZ`

**Find new listings:**
â†’ `current_scraped_data/NEW_CALIFORNIA_LISTINGS.csv`

**Setup automation:**
â†’ `docs/QUICK_START_MONTHLY_UPDATES.md`

**Import to WordPress:**
â†’ `wordpress_import/` or `current_scraped_data/`

**Check scraper progress:**
â†’ `tail -f /tmp/scrape_all_unlimited.log`

**Understand the system:**
â†’ `docs/SYSTEM_OVERVIEW.md`

## ğŸ“‹ File Types by Location

### CSVs

- `current_scraped_data/` - TODAY's scrape results
- `organized_csvs/` - Historical organized data
- `wordpress_import/` - Ready for import
- `california_expansion/` - CA project data
- `archive/test_csvs/` - Test/sample files

### Python Scripts

- Root: `scrape_all_states.py` (main scraper)
- `scrapers_active/` - Current working scrapers
- `monthly_scrapers/` - Automation system
- `data_analysis/` - Analysis tools
- `test_scripts/` - Test utilities
- `archive/old_scripts/` - Deprecated scripts

### Documentation

- Root: `README.md`, `memory.md`
- `docs/` - All guides and documentation
- Each subfolder has its own `README.md`

### Logs

- `/tmp/scrape_all_unlimited.log` - Current scraper
- `data/logs/` - System logs
- `archive/logs/` - Old logs

## ğŸ§¹ Cleanup Rules

### Keep in Root

- `README.md` - Project overview
- `memory.md` - AI session history
- `scrape_all_states.py` - Main active scraper
- `FOLDER_ORGANIZATION.md` - This file

### Archive These

- One-off scripts â†’ `archive/old_scripts/`
- Test/sample CSVs â†’ `archive/test_csvs/`
- Old logs â†’ `archive/logs/`
- Deprecated state fixes â†’ `archive/old_state_fixes/`

### Active Directories

- `current_scraped_data/` - Clear old after import
- `monthly_updates/` - Keep last 3 months
- `scrapers_active/` - Only working scrapers
- `docs/` - Keep all documentation

## ğŸ”„ Maintenance Schedule

**Daily:**

- Check scraper progress: `tail /tmp/scrape_all_unlimited.log`

**Weekly:**

- Clear old `current_scraped_data/` after import
- Review `monthly_updates/` for processing

**Monthly:**

- Archive old `monthly_updates/` (keep 3 months)
- Update `memory.md` with session notes
- Review and archive unused scripts

**Quarterly:**

- Clean `data_outputs/` of old intermediate files
- Review `organized_csvs/` for consolidation
- Update documentation

---

**Last Organized**: October 28, 2025  
**Organization Pattern**: Active files up front, archives in subfolders  
**Principle**: Everything has a place, nothing in root unless active
